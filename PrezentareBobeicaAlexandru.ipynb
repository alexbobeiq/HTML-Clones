{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTLM Clones detection\n",
    "\n",
    "The problem:\n",
    "This project is about clustering html documents that are similar from the perspective of a user opening them <br>\n",
    "We have 4 folders containing a different number of documents ad we want to extract from each of them the groups of clones. <br>\n",
    "\n",
    "First question that should be asked is: \"What does it mean that 2 websites are similar?\"<br>\n",
    "\n",
    "There are multiple anwers to this question but I'll focus on 2 of them:<br>\n",
    "1)  2 websites are simmilar if they look the same.<br>\n",
    "    The first thing that a user notices when landing on a webpage is its style as in color scheme, images, layout of the elements etc<br>\n",
    "2)  2 websites are similar if the information that they transmit is similar (the textual content is similar)<br>\n",
    "\n",
    "Analysing these 2 criteria is enough to decide if one site is the clone of another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping for information\n",
    "\n",
    "But before we get to make the final decision there are some steps to make. The first of them is to collect the raw information from the websites we want to classify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for text data\n",
    "\n",
    "First we'll define a fuction that given a parameter *path* for a specific document will extract all the text found in certain html elements such as p, li, div, h1 ect <br>\n",
    "For this we'll use the get_text function of the BeautifulSoup library. <br>\n",
    "Also, all the links, new lines and carriage returs will be removed from the text because they are irrelevant for the project's purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "def extract_text(path):\n",
    "    with open(path, \"r\", encoding=\"utf\") as file:\n",
    "        html = file.read()\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = text.replace('\\n', ' ').replace('\\r', '')\n",
    "\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for visual information\n",
    "\n",
    "Using the Selenium library we can open each page in a specific window size, take a screen shot of the whole page and then save each of the images in a folder to be processed later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "SCREENSHOT_DIR = \"screenshots4\"\n",
    "os.makedirs(SCREENSHOT_DIR, exist_ok=True)\n",
    "TIER1 = \"clones/tier1\"\n",
    "TIER2 = \"clones/tier2\"\n",
    "TIER3 = \"clones/tier3\"\n",
    "TIER4 = \"clones/tier4\"\n",
    "\n",
    "tier1_files = glob.glob(os.path.join(TIER1, \"*.html\"))\n",
    "tier2_files = glob.glob(os.path.join(TIER2, \"*.html\"))\n",
    "tier3_files = glob.glob(os.path.join(TIER3, \"*.html\"))\n",
    "tier4_files = glob.glob(os.path.join(TIER4, \"*.html\"))\n",
    "\n",
    "\n",
    "def take_screenshot(url, filename):\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        path = os.path.join(SCREENSHOT_DIR, filename)\n",
    "        driver.set_window_size(1200, 900)\n",
    "\n",
    "        driver.save_screenshot(path)\n",
    "        print(f\"screenshot taken: {path}\")\n",
    "        return path\n",
    "    except Exception as e:\n",
    "        print(f\"error: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def screenshots_for_tiers(directory):\n",
    "    for file in directory:\n",
    "        url = f\"file:///{os.path.abspath(file)}\"\n",
    "        domain = os.path.basename(file)\n",
    "        filename = domain.replace(\".\", \"_\").replace(\"_html\", \".png\")\n",
    "        take_screenshot(url, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Exctraction \n",
    "Now we have have to text and the screenshots, but this data is unusable for training a model, thus we need to process the data we aquired so far in order to get an array of numerical value that is representative for the dataset (a feature vector)<br>\n",
    "\n",
    "- For textual information there are numerous methods in NLP in feature extraction.<br>\n",
    "For this project we'll use TF-IDF Vectorization. <br>\n",
    "It outputs the importance of a word in a collection of documents. <br>\n",
    "how it does that? <br> It calculates The frequency of a term appearing in a document TF: <br>\n",
    "$$\n",
    "TF = \\frac{Nr \\ of \\ times \\ a \\ word \\ apears \\ in \\ document}{Nr \\ of \\ words \\ in \\ a \\ document}\n",
    "$$\n",
    "\n",
    "And the Inverse Document Frequency which is the total number of documents devided by the number of documents containing said term: <br>\n",
    "\n",
    "$$\n",
    "IDF = \\log {\\frac{Nr}{Nr \\ of \\ documents \\ containing \\ said \\ word}}\n",
    "$$\n",
    "\n",
    "Then, the TF-IDF score will just be the product of TF and IDF <br>\n",
    "\n",
    "There are some other alternatives, such as BoW(is simpler, creates a vector of word frequencies, but the representation si sparse), Hashing vectorizer(best for reducing the dimensionality of the feature matrix, it's best for large datasets, which is not the case.), or word-embeddings(computationally expensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import glob\n",
    "import string\n",
    "import numpy as np\n",
    "from skimage.feature import local_binary_pattern\n",
    "from scrape_text import extract_text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return text.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "def extract_tfidf_features(data):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "    tfidf_matrix = vectorizer.fit_transform(data.values())\n",
    "    return {key: tfidf_matrix[i].toarray()[0] for i, key in enumerate(data.keys())}\n",
    "\n",
    "doc = {\n",
    "    \"doc1\" : \"this is an example text\",\n",
    "    \"doc2\" : \"Another example of text\",\n",
    "    \"doc3\" : \"Example 3 of no meaning sentance\"\n",
    "    }\n",
    "\n",
    "print(extract_tfidf_features(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction for Screenshots\n",
    "\n",
    "For Images we'll use the Local Binary Pattern. <br>\n",
    "\n",
    "It assigns for each pixel a number in binary representation of 8 bits as follows: It looks at the neighbours of every pixel clockwise starting from the top left. If the neighbour is higher in intensity it appends a 1 to the number, otherwise a 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from skimage import feature, io, color\n",
    "def lbp_histogram(image, num_points=24, radius=3, bins=32):\n",
    "    lbp = local_binary_pattern(image, num_points, radius, method=\"uniform\")\n",
    "    hist, _ = np.histogram(lbp.ravel(), bins=bins, range=(0, bins), density=True)\n",
    "    return hist\n",
    "\n",
    "image = io.imread('https://pixelixe.com/docs/image-processing/grayscale-image-api.png')\n",
    "gray_image = color.rgb2gray(image)\n",
    "lbp_image = feature.local_binary_pattern(gray_image, 8, 1, method=\"uniform\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "axes[0].imshow(gray_image, cmap='gray')\n",
    "axes[0].set_title('Original Grayscale Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(lbp_image, cmap='gray')\n",
    "axes[1].set_title('LBP Image')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the combined feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "TIER1 = \"clones/tier1\"\n",
    "TIER2 = \"clones/tier2\"\n",
    "TIER3 = \"clones/tier3\"\n",
    "TIER4 = \"clones/tier4\"\n",
    "\n",
    "tier1_files = glob.glob(os.path.join(TIER1, \"*.html\"))\n",
    "tier2_files = glob.glob(os.path.join(TIER2, \"*.html\"))\n",
    "tier3_files = glob.glob(os.path.join(TIER3, \"*.html\"))\n",
    "tier4_files = glob.glob(os.path.join(TIER4, \"*.html\"))\n",
    "\n",
    "def extract_features(file):\n",
    "\n",
    "    if file == 1:\n",
    "        SCREENSHOT_DIR = \"screenshots1\"\n",
    "        tier = tier1_files\n",
    "        path = \"clones/tier1\\\\\"\n",
    "    elif file == 2:\n",
    "        SCREENSHOT_DIR = \"screenshots2\"\n",
    "        tier = tier2_files\n",
    "        path = \"clones/tier2\\\\\"\n",
    "    elif file == 3:\n",
    "        SCREENSHOT_DIR = \"screenshots3\"\n",
    "        tier = tier3_files\n",
    "        path = \"clones/tier3\\\\\"\n",
    "    elif file == 4:\n",
    "        SCREENSHOT_DIR = \"screenshots4\"\n",
    "        tier = tier4_files\n",
    "        path = \"clones/tier4\\\\\"\n",
    "        \n",
    "    text = {file: extract_text(file) for file in tier}\n",
    "    tfidf_features = extract_tfidf_features(text)\n",
    "\n",
    "    screenshots = glob.glob(os.path.join(SCREENSHOT_DIR, \"*.png\"))\n",
    "    texture_features = {}\n",
    "\n",
    "    for screenshot in screenshots:\n",
    "        image = cv2.imread(screenshot, cv2.IMREAD_GRAYSCALE)\n",
    "        histo = lbp_histogram(image)\n",
    "        texture_features[path + os.path.basename(screenshot).replace(\"_\", \".\").replace(\".png\", \".html\")] = histo\n",
    "\n",
    "    common_keys = set(tfidf_features.keys()) & set(texture_features.keys())\n",
    "    combined_features = {key: np.concatenate((tfidf_features[key], texture_features[key])) for key in common_keys}\n",
    "\n",
    "    df = pd.DataFrame.from_dict(combined_features, orient=\"index\")\n",
    "    shape_text = len(next(iter(tfidf_features.values())))\n",
    "    shape_img = len(next(iter(texture_features.values())))\n",
    "\n",
    "    df.to_csv(\"combined_features.csv\", index=True, header=False)\n",
    "\n",
    "    print(\"✅ Features saved to combined_features4.csv\")\n",
    "    return shape_text, shape_img, combined_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Model\n",
    "\n",
    "Now that we have the feature matrix we can finally train a model to cluster the websites <br>\n",
    "One of the simplest and most effective clustering algorithm is K-means. It works by randomly placing a predifined number of centroids K in the feature plain, then assigning the to that centroids the points closest to them, then moving the centroids to the geometric center of their points. <br>\n",
    "The process is repeated until the centroids don't move anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, load the feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from kneed import KneeLocator\n",
    "\n",
    "print(\"choose tier for clustering: 1, 2, 3 or 4\")\n",
    "\n",
    "tier = 1 # Modifica pentru a alege alt folder\n",
    "print(\"Extracting textual and image features...\")\n",
    "shape_text, shape_img, combined_features = extract_features(tier)\n",
    "feature_matrix = np.array(list(combined_features.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "One thing to consider is scaling the features. <br>\n",
    "Let's consider the next scenario: There are 2 websites identical in text, but completly diferent in aspect. Should they be grouped together or not? <br>\n",
    "Since it's not good to judge a book by its cover :))) (the information is more important than style) I think they should be grouped. To ensure this the text features will have a higher weight than the rest.<br>\n",
    "Also, we'll aply the standard scaler transformation before this on the entire matrix, to center and normalize the data on the origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_collums = np.arange(0, shape_text)\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(feature_matrix)\n",
    "scaled_features[:, text_collums] = scaled_features[:, text_collums] * 3\n",
    "valid_websites = [website for website in enumerate(combined_features.keys())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "One thing that can improve the computation speed drastically is aplying the PCA transform<br>\n",
    "This is used for dimension reduction, as it retains only the features that describe 95% of the variance of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "pca_features = pca.fit_transform(scaled_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the best k?\n",
    "\n",
    "One problem that K-means has is that you have to choose a predifined number of centroids before training it. But we don't know how many clusters there are before hand. <br>\n",
    "So what can we do?<br>\n",
    "There are technics for finding the best k. One of them is the *Elbow Method*. It works by looking at the graph of the inertia with respect to the number of clusters k, and choosing the k where the graph bends(has a high second derivative).<br>\n",
    "However, it doesn't work for all datasets. Some have an evolution of inertia almost linear, or an elbow is hard to find<br>\n",
    "For those, we can chose another method, such as the *Sillhouete Score*.<br>\n",
    "It measures how well-defined the clusters are by quantifying how similar each data point is to its own cluster compared to other clusters.\n",
    "A higher Silhouette Score means that clusters are well-separated and cohesive, while a lower score indicates that clusters are overlapping or poorly defined.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_scores = []\n",
    "inertia = []\n",
    "k_values = range(2, 20)\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(pca_features)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(pca_features, kmeans.labels_))\n",
    "    print(f\"K={k}, Inertia={kmeans.inertia_}, Silhouette Score={silhouette_scores[-1]}\")\n",
    "\n",
    "knee_locator = KneeLocator(k_values, inertia, curve=\"convex\", direction=\"decreasing\")\n",
    "optimal_k_elbow = knee_locator.elbow\n",
    "if optimal_k_elbow:\n",
    "    elbow_idx = k_values.index(optimal_k_elbow)\n",
    "    \n",
    "    second_derivative = np.diff(np.diff(inertia))\n",
    "    \n",
    "    if elbow_idx > 0 and elbow_idx < len(second_derivative):\n",
    "        elbow_strength = second_derivative[elbow_idx - 1]\n",
    "    else:\n",
    "        elbow_strength = 0\n",
    "    \n",
    "    inertia_drop = (inertia[elbow_idx - 1] - inertia[elbow_idx]) / inertia[elbow_idx - 1]\n",
    "\n",
    "    if elbow_strength > np.percentile(second_derivative, 75) and inertia_drop > 0.3:\n",
    "        optimal_k = optimal_k_elbow\n",
    "        print(f\"✅ Optimal K determined by Elbow Method: {optimal_k}\")\n",
    "    else:\n",
    "        optimal_k = k_values[np.argmax(silhouette_scores)]\n",
    "        print(f\"🔄 Switching to Silhouette Score: Optimal K = {optimal_k}\")\n",
    "else:\n",
    "    optimal_k = k_values[np.argmax(silhouette_scores)]\n",
    "    print(f\"🔄 No clear elbow detected. Using Silhouette Score: Optimal K = {optimal_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the graphs of Inertia and Silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, inertia, marker='o', linestyle='--', label=\"Inertia\")\n",
    "if optimal_k_elbow:\n",
    "    plt.axvline(x=optimal_k_elbow, color='red', linestyle='--', label=\"Elbow Point\")\n",
    "plt.xlabel(\"Number of Clusters (K)\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.title(\"Elbow Method for Optimal K\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, silhouette_scores, marker='o', linestyle='--', label=\"Silhouette Score\")\n",
    "plt.axvline(x=optimal_k, color='green', linestyle='--', label=\"Chosen K\")\n",
    "plt.xlabel(\"Number of Clusters (K)\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.title(\"Silhouette Method for Optimal K\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally Clustering\n",
    "Now that we found our number of clusters we can finally train our model and output the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(pca_features)\n",
    "\n",
    "for i, website in enumerate(valid_websites):\n",
    "    cluster_label = kmeans_labels[i]\n",
    "    print(f\"{website} -> Cluster {cluster_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model\n",
    "Taking a close look at the output, it can be noticed that the model performs well at detecting the main clusters of websites, however it is sensitive to some noise points (unique sites are misclasiffied with some big groups). <br>\n",
    "Also we can plot the first 2 principal components and take a look at the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_2d = PCA(n_components=2)\n",
    "reduced_features = pca_2d.fit_transform(pca_features)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "scatter = plt.scatter(reduced_features[:, 0], reduced_features[:, 1], \n",
    "                      c=kmeans_labels, cmap='viridis', s=50, edgecolors='k')\n",
    "\n",
    "color_bar = plt.colorbar(scatter)\n",
    "color_bar.set_label('Cluster Label')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('2D K-Means Clustering of Websites')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
